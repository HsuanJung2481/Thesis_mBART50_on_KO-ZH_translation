{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mBART50 - multilingual fine-tuning (SWRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data\n",
    "# Train data\n",
    "#Read the file\n",
    "with open(\"ko_train_SWRC.txt\", \"r\", encoding=\"utf-8\") as ko_train_SWRC:\n",
    "    ko_sentences = ko_train_SWRC.readlines()\n",
    "\n",
    "with open(\"ch_train_SWRC.txt\", \"r\", encoding=\"utf-8\") as zh_train_SWRC:\n",
    "    zh_sentences = zh_train_SWRC.readlines()\n",
    "\n",
    "#Remove \\n\n",
    "ko_sentences = [line.strip() for line in ko_sentences]\n",
    "zh_sentences = [line.strip() for line in zh_sentences]\n",
    "\n",
    "#Merge ko-sentence and zh-sentence into a df\n",
    "merged_df = pd.DataFrame({\n",
    "    'source': ko_sentences,\n",
    "    'target': zh_sentences\n",
    "})\n",
    "\n",
    "#Save as a new CSV\n",
    "merged_df.to_csv(\"ko_zh_train_dataset_SWRC.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Validation data\n",
    "#Read the file\n",
    "with open(\"ko_vali_SWRC.txt\", \"r\", encoding=\"utf-8\") as ko_validation:\n",
    "    ko_sentences = ko_validation.readlines()\n",
    "\n",
    "with open(\"ch_vali_SWRC.txt\", \"r\", encoding=\"utf-8\") as zh_validation:\n",
    "    zh_sentences = zh_validation.readlines()\n",
    "\n",
    "#Remove \\n\n",
    "ko_sentences = [line.strip() for line in ko_sentences]\n",
    "zh_sentences = [line.strip() for line in zh_sentences]\n",
    "\n",
    "#Check if row number is the same\n",
    "if len(ko_sentences) != len(zh_sentences):\n",
    "    raise ValueError(\"Not the same.\")\n",
    "\n",
    "#Merge ko-sentence and zh-sentence into a df\n",
    "merged_df = pd.DataFrame({\n",
    "    'source': ko_sentences,\n",
    "    'target': zh_sentences\n",
    "})\n",
    "\n",
    "#Save as a new CSV\n",
    "merged_df.to_csv(\"ko_zh_validation_dataset_SWRC.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Test data\n",
    "#Read the file\n",
    "with open(\"ko_test_SWRC.txt\", \"r\", encoding=\"utf-8\") as ko_test:\n",
    "    ko_sentences = ko_test.readlines()\n",
    "\n",
    "with open(\"ch_test_SWRC.txt\", \"r\", encoding=\"utf-8\") as zh_test:\n",
    "    zh_sentences = zh_test.readlines()\n",
    "\n",
    "#Remove \\n\n",
    "ko_sentences = [line.strip() for line in ko_sentences]\n",
    "zh_sentences = [line.strip() for line in zh_sentences]\n",
    "\n",
    "#Check if row number is the same\n",
    "if len(ko_sentences) != len(zh_sentences):\n",
    "    raise ValueError(\"Not the same.\")\n",
    "\n",
    "#Merge ko-sentence and zh-sentence into a df\n",
    "merged_df = pd.DataFrame({\n",
    "    'source': ko_sentences,\n",
    "    'target': zh_sentences\n",
    "})\n",
    "\n",
    "#Save as a new CSV\n",
    "merged_df.to_csv(\"ko_zh_test_dataset_SWRC.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label language codes (Same step of labeling japanese data, japanese language code: ja_XX)\n",
    "\n",
    "df = pd.read_csv('ko_zh_train_dataset_SWRC.csv')  \n",
    "\n",
    "# Add langauge code\n",
    "df['language'] = 'ko_KR'\n",
    "\n",
    "# Save\n",
    "df.to_csv('ko_zh_train_dataset_SWRC_languagecode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ja_ch_corpus with SWRC\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('ja_ch_trian_webcrawl_languagecode.csv')\n",
    "df2 = pd.read_csv('ko_zh_train_dataset_SWRC_languagecode.csv')\n",
    "\n",
    "df_merged = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df_merged.to_csv('SWRC_webcrawl_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "# Load dataset\n",
    "data_files = {\n",
    "    \"train\": \"SWRC_webcrawl_train.csv\",\n",
    "    \"validation\": \"SWRC_webcrawl_dev.csv.csv\",\n",
    "    \"test\": \"SWRC_webcrawl_test.csv\"\n",
    "}\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['source']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # Set the source language dynamically based on the input\n",
    "    if examples['language'][0] == \"ja_XX\":\n",
    "        tokenizer.src_lang = \"ja_XX\"\n",
    "    elif examples['language'][0] == \"ko_KR\":\n",
    "        tokenizer.src_lang = \"ko_KR\"\n",
    "\n",
    "    tokenizer.tgt_lang = \"zh_CN\"  # Set target language to Chinese\n",
    "\n",
    "    # Tokenize inputs and labels\n",
    "    model_inputs = tokenizer(inputs, max_length=200, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=200, truncation=True, padding='max_length')\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Disable WandB\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Set output directory\n",
    "output_dir = '/home/u542596/experiments/multilingual_fine_tune/SWRC_webcrawl'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=1500,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "train_results = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
