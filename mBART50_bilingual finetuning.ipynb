{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mBART50 - bilingual fine-tuning (SWRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data\n",
    "# Train data\n",
    "#Read the file\n",
    "with open(\"ko_train_SWRC.txt\", \"r\", encoding=\"utf-8\") as ko_train_SWRC:\n",
    "    ko_sentences = ko_train_SWRC.readlines()\n",
    "\n",
    "with open(\"ch_train_SWRC.txt\", \"r\", encoding=\"utf-8\") as zh_train_SWRC:\n",
    "    zh_sentences = zh_train_SWRC.readlines()\n",
    "\n",
    "#Remove \\n\n",
    "ko_sentences = [line.strip() for line in ko_sentences]\n",
    "zh_sentences = [line.strip() for line in zh_sentences]\n",
    "\n",
    "#Merge ko-sentence and zh-sentence into a df\n",
    "merged_df = pd.DataFrame({\n",
    "    'source': ko_sentences,\n",
    "    'target': zh_sentences\n",
    "})\n",
    "\n",
    "#Save as a new CSV\n",
    "merged_df.to_csv(\"ko_zh_train_dataset_SWRC.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Validation data\n",
    "#Read the file\n",
    "with open(\"ko_vali_SWRC.txt\", \"r\", encoding=\"utf-8\") as ko_validation:\n",
    "    ko_sentences = ko_validation.readlines()\n",
    "\n",
    "with open(\"ch_vali_SWRC.txt\", \"r\", encoding=\"utf-8\") as zh_validation:\n",
    "    zh_sentences = zh_validation.readlines()\n",
    "\n",
    "#Remove \\n\n",
    "ko_sentences = [line.strip() for line in ko_sentences]\n",
    "zh_sentences = [line.strip() for line in zh_sentences]\n",
    "\n",
    "#Check if row number is the same\n",
    "if len(ko_sentences) != len(zh_sentences):\n",
    "    raise ValueError(\"Not the same.\")\n",
    "\n",
    "#Merge ko-sentence and zh-sentence into a df\n",
    "merged_df = pd.DataFrame({\n",
    "    'source': ko_sentences,\n",
    "    'target': zh_sentences\n",
    "})\n",
    "\n",
    "#Save as a new CSV\n",
    "merged_df.to_csv(\"ko_zh_validation_dataset_SWRC.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Test data\n",
    "#Read the file\n",
    "with open(\"ko_test_SWRC.txt\", \"r\", encoding=\"utf-8\") as ko_test:\n",
    "    ko_sentences = ko_test.readlines()\n",
    "\n",
    "with open(\"ch_test_SWRC.txt\", \"r\", encoding=\"utf-8\") as zh_test:\n",
    "    zh_sentences = zh_test.readlines()\n",
    "\n",
    "#Remove \\n\n",
    "ko_sentences = [line.strip() for line in ko_sentences]\n",
    "zh_sentences = [line.strip() for line in zh_sentences]\n",
    "\n",
    "#Check if row number is the same\n",
    "if len(ko_sentences) != len(zh_sentences):\n",
    "    raise ValueError(\"Not the same.\")\n",
    "\n",
    "#Merge ko-sentence and zh-sentence into a df\n",
    "merged_df = pd.DataFrame({\n",
    "    'source': ko_sentences,\n",
    "    'target': zh_sentences\n",
    "})\n",
    "\n",
    "#Save as a new CSV\n",
    "merged_df.to_csv(\"ko_zh_test_dataset_SWRC.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import safe_open\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load datasets\n",
    "data_files = {\n",
    "    \"train\": \"ko_zh_train_dataset_donga.csv\",\n",
    "    \"validation\": \"ko_zh_validation_dataset_donga.csv\",\n",
    "    \"test\": \"ko_zh_test_dataset_donga.csv\"\n",
    "}\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Load the base model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "empty_state_dict = {}\n",
    "model.load_state_dict(empty_state_dict, strict=False)\n",
    "model.to(device)\n",
    "\n",
    "# Set source and target languages\n",
    "tokenizer.src_lang = \"ko_KR\"  # source language\n",
    "tokenizer.tgt_lang = \"zh_CN\"  # Chinese as target language\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    if 'source' not in examples or 'target' not in examples:\n",
    "        raise ValueError(\"Missing 'source' or 'target' in examples.\")\n",
    "\n",
    "    inputs = [ex for ex in examples['source'] if ex is not None]\n",
    "    targets = [ex for ex in examples['target'] if ex is not None]\n",
    "\n",
    "    if not inputs or not targets:\n",
    "        raise ValueError(\"Inputs or targets cannot be empty or None.\")\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=200, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=200, truncation=True, padding='max_length')\n",
    "\n",
    "    if labels is None or 'input_ids' not in labels:\n",
    "        raise ValueError(\"Tokenization of targets failed, resulting in None or missing 'input_ids'.\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Disable WandB\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Set output directory\n",
    "output_dir = '/home/u542596/experiments/bilingual_fine_tune/SWRC'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=1500,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Fine tune\n",
    "train_results = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
