{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mBART50 - BLEU and COMET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = load_dataset('csv', data_files='ko_zh_test_dataset_SWRC.csv')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Define the translation function\n",
    "def generate_translation(examples, model, batch_size = 16, max_length=200):\n",
    "    target_language_code = \"zh_CN\"\n",
    "    formatted_inputs = [f\"ko_KR {sentence} </s>\" for sentence in examples['source']]\n",
    "    encoded_inputs = tokenizer(formatted_inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    # Use batching to handle large numbers of sentences at once\n",
    "    all_generated_texts = []\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    attention_mask = encoded_inputs['attention_mask']\n",
    "\n",
    "    # Split input into smaller batches based on batch_size\n",
    "    num_batches = (len(input_ids) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(input_ids))\n",
    "\n",
    "        # Get the current batch\n",
    "        input_ids_batch = input_ids[start_idx:end_idx].to(model.device)\n",
    "        attention_mask_batch = attention_mask[start_idx:end_idx].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids_batch,\n",
    "                attention_mask=attention_mask_batch,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[target_language_code]\n",
    "            )\n",
    "        # Decode the generated ids to text\n",
    "        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_generated_texts.extend(generated_texts)\n",
    "\n",
    "    return {\"generated_text\": all_generated_texts}\n",
    "\n",
    "# Specify the checkpoint to process\n",
    "checkpoint_dir = '/home/u542596/experiments/bilingual_fine_tune/SWRC'\n",
    "checkpoints = [name for name in os.listdir(checkpoint_dir) if name.startswith(\"checkpoint-\")]\n",
    "\n",
    "# GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Process each checkpoint\n",
    "for checkpoint_name in checkpoints:\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "    output_file = f'generated_translation_{checkpoint_name}_SWRC.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        batch = []\n",
    "        for idx, source in enumerate(test_dataset['train']['source']):\n",
    "            batch.append(source)\n",
    "\n",
    "            if len(batch) == batch_size or (idx + 1) == len(test_dataset['train']['source']):\n",
    "                generated_translation = generate_translation({\"source\": batch}, model, batch_size=batch_size)\n",
    "                for translation in generated_translation['generated_text']:\n",
    "                    f.write(translation + '\\n')\n",
    "\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    print(f\"Processed {idx + 1} sentences for {checkpoint_name}\")\n",
    "\n",
    "                batch.clear()\n",
    "\n",
    "    print(f\"Finished generating translations for {checkpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-based segmentation (both reference and generated translation)\n",
    "import pandas as pd\n",
    "\n",
    "input_file = '/home/u542596/experiments/bilingual_fine_tune/BLEU_and_COMET/generated_translation_checkpoint_16030.txt'\n",
    "output_file = '/home/u542596/experiments/bilingual_fine_tune/BLEU_and_COMET/generated_translation_checkpoint_16030_seg.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    sentences = infile.readlines()\n",
    "\n",
    "# character segment function\n",
    "def char_tokenize(text):\n",
    "    return ' '.join(list(text.strip()))\n",
    "\n",
    "# character-based segment every sentence\n",
    "char_segmented_sentences = [char_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# create a df and save to csv\n",
    "df = pd.DataFrame({'segmented_sentences': char_segmented_sentences})\n",
    "df.to_csv(output_file, index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BLEU script\n",
    "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sacrebleu\n",
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU\n",
    "python compute-bleu.py ko_zh_test_dataset_SWRC_seg.txt  generated_translation_checkpoint_16030_seg.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use the segmented translation\n",
    "# COMET\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "# Read sentence\n",
    "with open('korean_original_SWRC_test.txt', 'r', encoding='utf-8') as src_file, \\\n",
    "     open('generated_translation_checkpoint_16030.txt', 'r', encoding='utf-8') as mt_file, \\\n",
    "     open('chinese_original_SWRC_test.txt.txt', 'r', encoding='utf-8') as ref_file:\n",
    "\n",
    "    src_lines = src_file.readlines()\n",
    "    mt_lines = mt_file.readlines()\n",
    "    ref_lines = ref_file.readlines()\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    {\n",
    "        \"src\": src.strip(),\n",
    "        \"mt\": mt.strip(),\n",
    "        \"ref\": ref.strip()\n",
    "    }\n",
    "    for src, mt, ref in zip(src_lines, mt_lines, ref_lines)\n",
    "]\n",
    "\n",
    "# Sentence-level COMET\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "\n",
    "# Set output dir\n",
    "output_dir = 'comet'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"comet-score_generated_translation_checkpoint_16030.txt\")\n",
    "\n",
    "# Write scores to the file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for i, score in enumerate(model_output[\"scores\"]):\n",
    "        f.write(f\"Sentence {i}: {score}\\n\")\n",
    "    f.write(f\"Overall COMET Score: {model_output['system_score']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
