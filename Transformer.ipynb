{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer: following tutorial of OpenNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory and clone the Github MT-Preparation repository\n",
    "git clone https://github.com/ymoslem/MT-Preparation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the requirements\n",
    "pip install -r MT-Preparation/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting sentences in Korean and Chinese datasets\n",
    "import pandas as pd\n",
    "\n",
    "input_file = '/home/u542596/experiments/bilingual_fine_tune/BLEU_and_COMET/korean_original_SWRC_train.txt'\n",
    "output_file = '/home/u542596/experiments/bilingual_fine_tune/BLEU_and_COMET/korean_original_SWRC_train_seg.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    sentences = infile.readlines()\n",
    "\n",
    "# character segment function\n",
    "def char_tokenize(text):\n",
    "    return ' '.join(list(text.strip()))\n",
    "\n",
    "# character-based segment every sentence\n",
    "char_segmented_sentences = [char_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# create a df and save to csv\n",
    "df = pd.DataFrame({'segmented_sentences': char_segmented_sentences})\n",
    "df.to_csv(output_file, index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: korean_original_SWRC_train_seg.txt\n",
    "Target: chinese_original_SWRC_train_seg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence piece\n",
    "python MT-Preparation/subwording/1-train_unigram.py korean_original_SWRC_train_seg.txt chinese_original_SWRC_train_seg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subword\n",
    "python MT-Preparation/subwording/2-subword.py source.model target.model Source: korean_original_SWRC_train_seg.txt chinese_original_SWRC_train_seg.txt\n",
    "\n",
    "#Output\n",
    "#Done subwording the source file! Output: korean_original_SWRC_train_seg.txt.subword\n",
    "#Done subwording the target file! Output: chinese_original_SWRC_train_seg.txt.subword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not follow splitting part since we need to use exactly the same training, dev, and test sets with prior work. Therefore, just repeat steps from segmentation to subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "pip install OpenNMT-py\n",
    "\n",
    "# Create the YAML configuration file\n",
    "# Change hyperparameters based on prior work: https://arxiv.org/pdf/1911.11008\n",
    "\n",
    "config = '''# config.yaml\n",
    "## Where the samples will be written\n",
    "save_data: run\n",
    "# Training files\n",
    "data:\n",
    "    corpus_1:\n",
    "        path_src: korean_original_SWRC.txt-filtered.ko.subword.train\n",
    "        path_tgt: ch_original_SWRC_segmented.txt-filtered.ch.subword.train\n",
    "        transforms: [filtertoolong]\n",
    "    valid:\n",
    "        path_src: korean_original_SWRC.txt-filtered.ko.subword.dev\n",
    "        path_tgt: ch_original_SWRC_segmented.txt-filtered.ch.subword.dev\n",
    "        transforms: [filtertoolong]\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: run/source.vocab\n",
    "tgt_vocab: run/target.vocab\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 50000\n",
    "tgt_vocab_size: 50000\n",
    "# Filter out source/target longer than n if [filtertoolong] enabled\n",
    "src_seq_length: 50\n",
    "src_seq_length: 50\n",
    "# Tokenization options\n",
    "src_subword_model: source.model\n",
    "tgt_subword_model: target.model\n",
    "# Where to save the log file and the output models/checkpoints\n",
    "log_file: train.log\n",
    "save_model: models/model.transformer\n",
    "\n",
    "# Stop training if it does not imporve after n validations\n",
    "#early_stopping: 4\n",
    "# Default: 5000 - Save a model checkpoint for each n\n",
    "save_checkpoint_steps: 5000\n",
    "# To save space, limit checkpoints to last n\n",
    "# keep_checkpoint: 3\n",
    "seed: 3435\n",
    "# Default: 100000 - Train the model to max n steps \n",
    "# Increase to 200000 or more for large datasets\n",
    "# For fine-tuning, add up the required steps to the original steps\n",
    "train_steps: 100000\n",
    "\n",
    "# Default: 10000 - Run validation after n steps\n",
    "valid_steps: 5000\n",
    "\n",
    "# Default: 4000 - for large datasets, try up to 8000\n",
    "warmup_steps: 8000\n",
    "report_every: 100\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "# Batching\n",
    "bucket_size: 262144\n",
    "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"tokens\"\n",
    "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 4096\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "# Optimization\n",
    "model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "learning_rate: 2\n",
    "# warmup_steps: 8000\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.98\n",
    "decay_method: \"noam\"\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "enc_layers: 6\n",
    "dec_layers: 6\n",
    "heads: 8\n",
    "hidden_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "''' \n",
    "with open(\"config.yaml\", \"w+\") as config_yaml: \n",
    "    config_yaml.write(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create voc\n",
    "onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "onmt_train -config config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation\n",
    "# Choose the best checkpoint: model.transformer_step_3000.pt\n",
    "onmt_translate -model models/model.transformer_step_3000.pt -src korean_original_SWRC_train_seg.txt.subword -output ch.SWRC_3000.translated -gpu 0 -min_length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 lines of the translation file \n",
    "head -n 5 ch.SWRC_3000.translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desubword the translation file \n",
    "python MT-Preparation/subwording/3-desubword.py target.model ch.SWRC_3000.translated\n",
    "\n",
    "# Output: ch.SWRC_3000.translated.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desubword the target file (reference) of the test dataset\n",
    "python3 MT-Preparation/subwording/3-desubword.py target.model chinese_original_SWRC_train_seg.txt.subword\n",
    "\n",
    "# Output: chinese_original_SWRC_train_seg.txt.subword.desubword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BLEU script\n",
    "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sacrebleu\n",
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU\n",
    "python compute-bleu.py ch_subword.test.desubword  ch.SWRC_100000.translated.desubword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove space -CH\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import os\n",
    "\n",
    "file_path = '/home/u542596/experiments/bilingual_fine_tune/BLEU_and_COMET/ch.SWRC_3000.translated'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "lines_no_spaces = [line.replace(\" \", \"\").strip() for line in lines]\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines_no_spaces))\n",
    "\n",
    "# COMET\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "# Read sentence\n",
    "with open('korean_original_SWRC_train.txt', 'r', encoding='utf-8') as src_file, \\\n",
    "     open('ch.SWRC_3000.translated', 'r', encoding='utf-8') as mt_file, \\\n",
    "     open('chinese_original_SWRC_train.txt.txt', 'r', encoding='utf-8') as ref_file:\n",
    "\n",
    "    src_lines = src_file.readlines()\n",
    "    mt_lines = mt_file.readlines()\n",
    "    ref_lines = ref_file.readlines()\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    {\n",
    "        \"src\": src.strip(),\n",
    "        \"mt\": mt.strip(),\n",
    "        \"ref\": ref.strip()\n",
    "    }\n",
    "    for src, mt, ref in zip(src_lines, mt_lines, ref_lines)\n",
    "]\n",
    "\n",
    "# Sentence-level COMET\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "\n",
    "# Set output dir\n",
    "output_dir = 'comet'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"comet-score_ch.SWRC_3000.translated\")\n",
    "\n",
    "# Write scores to the file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for i, score in enumerate(model_output[\"scores\"]):\n",
    "        f.write(f\"Sentence {i}: {score}\\n\")\n",
    "    f.write(f\"Overall COMET Score: {model_output['system_score']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
